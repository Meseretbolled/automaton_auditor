import json
import os
from typing import Dict, List, Tuple

from src.state import AgentState, Evidence, JudicialOpinion, JudgeName


# ----------------------------
# Helpers
# ----------------------------

def _load_rubric(path: str = "rubric.json") -> List[Dict]:
    if not os.path.exists(path):
        raise FileNotFoundError(
            f"Missing {path}. Create it in the repo root (same level as src/)."
        )
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data.get("criteria", [])


def _flatten_evidence(evidences: Dict[str, List[Evidence]]) -> List[Tuple[str, Evidence]]:
    """
    Returns list of (evidence_id, Evidence) where evidence_id is like 'repo_detective:0'
    """
    flat: List[Tuple[str, Evidence]] = []
    for source, items in evidences.items():
        for i, ev in enumerate(items):
            flat.append((f"{source}:{i}", ev))
    return flat


def _evidence_brief(evidences: Dict[str, List[Evidence]], max_items: int = 10) -> str:
    """
    Create a compact text summary of evidence for the judge prompt.
    """
    flat = _flatten_evidence(evidences)
    flat = flat[:max_items]

    lines = []
    for ev_id, ev in flat:
        status = "FOUND" if ev.found else "MISSING/FAIL"
        line = f"- [{ev_id}] {status} | goal={ev.goal} | location={ev.location} | conf={ev.confidence:.2f}"
        lines.append(line)

    if not lines:
        return "No evidence provided."

    return "\n".join(lines)


def _choose_citations(evidences: Dict[str, List[Evidence]], limit: int = 3) -> List[str]:
    """
    Choose evidence IDs to cite. Prefer negative evidence (found=False), then highest confidence.
    """
    flat = _flatten_evidence(evidences)

    negatives = [(eid, ev) for eid, ev in flat if not ev.found]
    positives = [(eid, ev) for eid, ev in flat if ev.found]

    negatives.sort(key=lambda x: x[1].confidence, reverse=True)
    positives.sort(key=lambda x: x[1].confidence, reverse=True)

    chosen = [eid for eid, _ in negatives[:limit]]
    if len(chosen) < limit:
        chosen += [eid for eid, _ in positives[: (limit - len(chosen))]]

    return chosen


def _llm_available() -> bool:
    return bool(os.getenv("GROQ_API_KEY", "").strip())


def _get_llm():
    from langchain_groq import ChatGroq

    model = os.getenv("GROQ_MODEL", "llama-3.3-70b-versatile")

    return ChatGroq(
        model=model,
        temperature=float(os.getenv("JUDGE_TEMPERATURE", "0.2"))
    )


def _judge_prompt(judge: JudgeName, criterion: Dict, evidence_text: str) -> str:
    """
    Different personas: Prosecutor (strict), Defense (generous), TechLead (practical).
    """
    cid = criterion["id"]
    cname = criterion.get("name", cid)
    cdesc = criterion.get("description", "")

    persona = {
        "Prosecutor": (
            "You are the Prosecutor. Be skeptical and strict. "
            "Assume corners were cut unless evidence proves otherwise. "
            "Penalize missing requirements, security issues, and vague claims."
        ),
        "Defense": (
            "You are the Defense. Be fair and generous. "
            "Give credit for partial implementations and clear intent. "
            "If evidence is incomplete, suggest what would complete it."
        ),
        "TechLead": (
            "You are the Tech Lead. Be practical and engineering-focused. "
            "Prioritize correctness, maintainability, and system design. "
            "Reward clean architecture and strong evidence."
        ),
    }[judge]

    scoring_rules = (
        "Score from 1 to 5:\n"
        "1 = fails/no evidence\n"
        "2 = weak / major gaps\n"
        "3 = acceptable / partial\n"
        "4 = strong\n"
        "5 = excellent / exemplary\n"
        "Your argument MUST reference the evidence list. Do not invent files or facts."
    )

    return f"""
{persona}

Criterion:
- id: {cid}
- name: {cname}
- description: {cdesc}

Evidence (subset):
{evidence_text}

{scoring_rules}

Return a JSON object that matches this schema:
{{"judge": "{judge}", "criterion_id": "{cid}", "score": <1-5>, "argument": "<short>", "cited_evidence": ["repo_detective:0", ...]}}
""".strip()


def _deterministic_fallback(judge: JudgeName, criteria: List[Dict], evidences: Dict[str, List[Evidence]]) -> List[JudicialOpinion]:
    """
    If no API key, still return structured opinions so the pipeline runs.
    This is NOT your final quality—just a safe fallback.
    """
    total = sum(len(v) for v in evidences.values())
    has_fail = any((not ev.found) for _, ev in _flatten_evidence(evidences))

    base = 3
    if total == 0:
        base = 1
    elif has_fail and judge == "Prosecutor":
        base = 2
    elif not has_fail and judge == "Defense":
        base = 4

    cites = _choose_citations(evidences, limit=3)

    opinions: List[JudicialOpinion] = []
    for c in criteria:
        opinions.append(
            JudicialOpinion(
                judge=judge,
                criterion_id=c["id"],
                score=base,
                argument=f"(Fallback) Evidence_count={total}, has_failures={has_fail}. Replace with LLM judging for final submission.",
                cited_evidence=cites,
            )
        )
    return opinions


def _run_judge(judge: JudgeName, state: AgentState) -> Dict:
    evidences = state.get("evidences", {})
    criteria = _load_rubric()

    if not criteria:
        raise ValueError("rubric.json loaded but contains no 'criteria'.")

    # LLM not available → fallback
    if not _llm_available():
        opinions = _deterministic_fallback(judge, criteria, evidences)
        return {"opinions": opinions}

    llm = _get_llm()

    evidence_text = _evidence_brief(evidences, max_items=int(os.getenv("MAX_EVIDENCE_FOR_JUDGES", "12")))

    opinions: List[JudicialOpinion] = []
    for criterion in criteria:
        prompt = _judge_prompt(judge, criterion, evidence_text)

        # Structured output directly into your Pydantic model
        structured = llm.with_structured_output(JudicialOpinion)
        opinion: JudicialOpinion = structured.invoke(prompt)

        # If model forgot citations, inject reasonable ones (defensive)
        if not opinion.cited_evidence:
            opinion.cited_evidence = _choose_citations(evidences, limit=3)

        opinions.append(opinion)

    return {"opinions": opinions}


# ----------------------------
# LangGraph Nodes
# ----------------------------

def prosecutor_judge(state: AgentState):
    return _run_judge("Prosecutor", state)


def defense_judge(state: AgentState):
    return _run_judge("Defense", state)


def techlead_judge(state: AgentState):
    return _run_judge("TechLead", state)


